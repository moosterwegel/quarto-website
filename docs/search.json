[
  {
    "objectID": "posts/impute-and-remove-technical-variation-metabolomics/index.html",
    "href": "posts/impute-and-remove-technical-variation-metabolomics/index.html",
    "title": "Imputing nondetects and removing technical variation in left-censored metabolomic data",
    "section": "",
    "text": "The field of environmental epidemiology has made significant investments in untargeted (also known as nontargeted) metabolomics over the past few years, to gain deeper insights into exposure levels, biological mechanisms, and their relationship to disease. These investments are starting to pay dividends by enabling larger studies, but this scale-up comes with its own challenges. With sample sizes growing substantially, processing all study samples in a single batch is no longer feasible, forcing analyses to span longer periods. These extended timeframes introduce greater variation in technical, non-biological factors that contaminate the data with unwanted variation. Additionally, as I’ve discussed in a previous post, this metabolomic data is often left-censored, creating further analytical challenges. In this post, I’ll share a method I developed that handles both the imputation of nondetects and the removal of technical variation in one and the same model1."
  },
  {
    "objectID": "posts/impute-and-remove-technical-variation-metabolomics/index.html#footnotes",
    "href": "posts/impute-and-remove-technical-variation-metabolomics/index.html#footnotes",
    "title": "Imputing nondetects and removing technical variation in left-censored metabolomic data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf your research question pertains directly to the exposure or metabolite level itself – rather than its relationship as a predictor for an outcome – then imputation is not necessary of course, because you can directly use the fitted left-censored model.↩︎\nFast, robust, frequentist left-censored multilevel models are not really possible/available.↩︎\nIn discussions I get the sense that many people think that the goal of imputation in metabolomics is to get as close as possible to the value that we would have observed with unlimited instrument sensitivity. While that’s welcome, for valid downstream inference with censored data it’s more important to accurately represent the uncertainty associated with those unobserved values.↩︎\nI think some kind of Gibbs sampler or some chained equation is the best we can do.↩︎\nI’m not sure what’s better (in this setup): a model with random slopes that possibly doesn’t fully converge or some 20 fixed effect interaction terms (with potentially some sparsity inducing horseshoe or LASSO priors). I guess putting (very) strong priors on the random slopes is the most logical option.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Oosterwegel -- blog ( ._.)",
    "section": "",
    "text": "Year\n\n\nTitle\n\n\nMonth & Day\n\n\n\n\n\n\n2025\n\n\nThe advantages of conditional logistic regression in (metabol)omic matched case-control studies\n\n\nMay 5\n\n\n\n\n\n\nImputing nondetects and removing technical variation in left-censored metabolomic data\n\n\nApril 28\n\n\n\n\n\n\nA guide to fitting left-censored models with R\n\n\nApril 21\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Max Oosterwegel",
    "section": "",
    "text": "I am a PhD student in environmental epidemiology at The Institute for Risk Assessment Sciences at Utrecht University. In this PhD I work with untargeted / nontargeted metabolomic data to study respiratory health outcomes (think asthma, COPD, lung function) across the life course. This PhD is financed by the EXPANSE and EXPOSOME-NL consortia which are part of a broader effort to systematically characterize the environment and its exposures to humans on a large scale (dubbed the exposome). I originally did my bachelor’s in artificial intelligence and a master in data science before pivoting to (environmental) epidemiology."
  },
  {
    "objectID": "posts/conditional-logistic-regression-omics/index.html",
    "href": "posts/conditional-logistic-regression-omics/index.html",
    "title": "The advantages of conditional logistic regression in (metabol)omic matched case-control studies",
    "section": "",
    "text": "Conditional logistic regression1 is a classic tool in an epidemiologist’s toolkit. It’s usually used to analyse matched case-control studies. However, popular methodological guidance has either explicitly advocated for ‘breaking’ the matching and using ordinary logistic models instead, or has sometimes been interpreted that way, with the justification being potential small precision gains (Pearce (2016)). In this blog post I want to demonstrate the advantages that conditional logistic regression can have in well-designed matched case-control studies of omic data that contain a substantial amount of technical variation."
  },
  {
    "objectID": "posts/conditional-logistic-regression-omics/index.html#stratum-specific",
    "href": "posts/conditional-logistic-regression-omics/index.html#stratum-specific",
    "title": "The advantages of conditional logistic regression in (metabol)omic matched case-control studies",
    "section": "Unconditional logistic regression: dummy variables",
    "text": "Unconditional logistic regression: dummy variables\nOne way to handle the bias from the matching process is by including dummy variables for all matched sets, effectively fitting a stratum-specific logistic regression model. In this model, the probability (\\(\\pi\\)) of being a case within matched set \\(k\\) depends on the predictors \\(x\\) and a unique intercept \\(\\alpha_k\\) for that specific set:\n\\[\n\\pi_k(\\mathbf{x})=\\frac{\\mathrm{e}^{\\alpha_k+\\boldsymbol{\\beta}^{\\prime} \\mathbf{x}}}{1+\\mathrm{e}^{\\alpha_k+\\boldsymbol{\\beta}^{\\prime} \\mathbf{x}}}\n\\] \\(\\alpha_k\\) represents the baseline log-odds of being a case that are specific to stratum \\(k\\), capturing the effects of the matching factors and any other unmeasured matching set characteristics. \\(\\beta\\) represents the log-odds ratios for the predictors \\(x\\), assumed to be constant across strata (Hosmer, Lemeshow, and Sturdivant (2013)).\nThis is somewhat feasible for large matching sets (i.e. many controls per case). However, such a fully stratified likelihood will need (\\(n/2 - 1\\)) extra parameters for 1:1 matched sets. This will lead to bias when using maximum likelihood because the number of parameters increases at the same rate as the sample size (see Breslow and Day 1980, sec. 7.1; also Hosmer, Lemeshow, and Sturdivant 2013, chap. 7)."
  },
  {
    "objectID": "posts/conditional-logistic-regression-omics/index.html#unconditional-logistic-regression-breaking-matching",
    "href": "posts/conditional-logistic-regression-omics/index.html#unconditional-logistic-regression-breaking-matching",
    "title": "The advantages of conditional logistic regression in (metabol)omic matched case-control studies",
    "section": "Unconditional logistic regression: breaking matching",
    "text": "Unconditional logistic regression: breaking matching\nBecause of this bias, unconditional logistic regression with dummy variables is rarely used in epidemiology. More common is an unconditional logistic regression where the matching set indicators are not explicitly modeled. Instead, the matching is broken and the variables that were used for matching are added as predictor to the logistic model. Matched sets indicators are thus omitted. This approach also utilizes contrasts between matched pairs and pairs with only a control or case in it still contribute to the analysis."
  },
  {
    "objectID": "posts/conditional-logistic-regression-omics/index.html#conditional-logistic-regression",
    "href": "posts/conditional-logistic-regression-omics/index.html#conditional-logistic-regression",
    "title": "The advantages of conditional logistic regression in (metabol)omic matched case-control studies",
    "section": "Conditional logistic regression",
    "text": "Conditional logistic regression\nAnother approach is conditional logistic regression (CLR). Instead of trying to estimate the potentially numerous stratum-specific intercepts (\\(\\alpha_k\\)) from the dummy variable model, CLR treats them as nuisance parameters and disregards their estimation such that we can use methods for conditional inference to create a likelihood function. After some derivation2, the conditional likelihood for the matched set \\(k\\) looks as follows for a 1:1 matched study design:\n\\[\nl_k(\\boldsymbol{\\beta})=\\frac{\\mathrm{e}^{\\beta^{\\prime} \\mathbf{x}_{1 k}}}{\\mathrm{e}^{\\beta^{\\prime} \\mathbf{x}_{1 k}}+\\mathrm{e}^{\\beta^{\\prime} \\mathbf{x}_{0 k}}} .\n\\] Where \\(x_1\\) and \\(x_0\\) correspond to the data from the case and control of a matched set \\(k\\) respectively, and \\(\\beta\\) is the only unknown parameter (your exposure for example). In other words, this conditional likelihood is the probability that, within matched set \\(k\\), the case is in fact the case if we assume a stratum-specific logistic regression model. The full conditional likelihood can be obtained here by taking the product of the likelihoods for each matched set (or summing the log-likelihoods of course).\nThe important thing to remember for my argument is that – contrary to logistic regression where we break the matching – the effect of a predictor is only measured relative to the values in its matched set rather than relative to all values of the predictor (Hosmer, Lemeshow, and Sturdivant (2013)). In standard epidemiology training, this is expressed as ‘only disconcordant pairs contribute information to the analysis’. This is also why such training mentions that you cannot analyse factors that were matched on – these values are constant within a pair and this estimator is thus undefined.\nIn R you can easily fit a CLR by writing survival::clogit(is_case ~ 1 + survival::strata(pair) + x, data = df)3."
  },
  {
    "objectID": "posts/conditional-logistic-regression-omics/index.html#footnotes",
    "href": "posts/conditional-logistic-regression-omics/index.html#footnotes",
    "title": "The advantages of conditional logistic regression in (metabol)omic matched case-control studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn other fields also known as the hypergeometric-normal model, or as a ‘fixed-effects logit model for panel data’.↩︎\nWhen deriving this conditional likelihood from the stratum-specific logistic model (which includes \\(\\alpha\\)) the \\(\\alpha_k\\) term (representing the combined baseline risk and effects of all factors constant within matching set \\(k\\), including matching variables) appears in both the numerator and denominator and thus mathematically cancels out. This cancellation is why we often say these matching set-constant effects are “absorbed” into a cluster-specific intercept; they are removed from the likelihood equation that needs to be solved. See Hosmer, Lemeshow, and Sturdivant (2013), chap. 7 for more details on this derivation.↩︎\nUnder the hood, this function is a wrapper around coxph because the log likelihood of a conditional logistic regression is the same as a Cox model with a specific data structure. Apparently, you can also – in some cases (?) – fit a conditional logistic regression model using a generalized linear mixed model with particular random intercepts structure.↩︎"
  },
  {
    "objectID": "posts/left-censored-models-showcase/index.html",
    "href": "posts/left-censored-models-showcase/index.html",
    "title": "A guide to fitting left-censored models with R",
    "section": "",
    "text": "In environmental epidemiology – and environmental studies more generally – we often aim to detect trace levels of pollutants, metabolites, or biomarkers. Be it in a biospecimen like blood and urine or in environmental media such as soil, water, and air. This can be done with liquid/gas chromatography mass spectrometry instruments among others. Even though these instruments are very sensitive these days, they will sometimes hit their detection limits. This blog posts will go briefly go over on how to handle such data statistically speaking, and what options are available in R to fit these models."
  },
  {
    "objectID": "posts/left-censored-models-showcase/index.html#multilevel-left-censored-models",
    "href": "posts/left-censored-models-showcase/index.html#multilevel-left-censored-models",
    "title": "A guide to fitting left-censored models with R",
    "section": "Multilevel left-censored models",
    "text": "Multilevel left-censored models\nSometimes your data is more complicated and you want to specify different levels of your data. For example: a random intercept that specifies what subject or batch an observation belongs to. Your R package options are more limited then, as the frequentist approach now needs to additionally integrate over the random effects in the definition of the marginal likelihood.\nThe {survival} model does not support such models while {censReg} supports only a simple two-level random intercept model:\n\nsamples_p_batch &lt;- 100\nnr_batches &lt;- n/samples_p_batch\nsd_batch &lt;- 2\n\nre_batch &lt;- rep(rnorm(nr_batches, 0, sd_batch), each = samples_p_batch)\ny &lt;-  rnorm(n, x * b_1 + re_batch, sd)\n\ncensor_point &lt;- quantile(y, prop_censored)\ncensored &lt;- y &lt;= censor_point\ny_obs &lt;- ifelse(censored, censor_point, y)\n\ndf &lt;- tibble::tibble(y, y_obs, x, censor_point, \n                     index = rep(1:samples_p_batch, times = nr_batches),\n                     batch_id = rep(1:nr_batches, each = samples_p_batch))\n\n\n{censReg}\nTo get such a two-level random intercept model to work in {censReg} we have to create a {plm} object first6\n\npData &lt;- plm::pdata.frame(df, c( \"batch_id\", \"index\"))\n\nm &lt;- censReg::censReg(y_obs ~ 1 + x, \n                      data = pData, \n                      method = \"BHHH\", \n                      left = min(df$y_obs), right = Inf)\nsummary(m)\n\n\nCall:\ncensReg::censReg(formula = y_obs ~ 1 + x, left = min(df$y_obs), \n    right = Inf, data = pData, method = \"BHHH\")\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n          2000            600           1400              0 \n\nCoefficients:\n             Estimate Std. error t value  Pr(&gt; t)    \n(Intercept) -0.091705   0.004090 -22.423  &lt; 2e-16 ***\nx            0.083038   0.016855   4.927 8.37e-07 ***\nlogSigmaMu   0.454964   0.002405 189.172  &lt; 2e-16 ***\nlogSigmaNu  -0.474995   0.007687 -61.794  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBHHH maximisation, 150 iterations\nReturn code 4: Iteration limit exceeded (iterlim)\nLog-likelihood: -1475.026 on 4 Df\n\n\n\n\n{GLMMadaptive}\nOne other frequentist package available for your left-censored models is {GLMMadaptive}. In addition to your observed outcome, you need to specify a vector with an indicator variable that describes whether the observation was censored:\n\nm &lt;- GLMMadaptive::mixed_model(cbind(y_obs, censored) ~ 1 + x, \n                               random = ~ 1|batch_id, \n                               data = df,\n                               family = GLMMadaptive::censored.normal())\nsummary(m)\n\n\nCall:\nGLMMadaptive::mixed_model(fixed = cbind(y_obs, censored) ~ 1 + \n    x, random = ~1 | batch_id, data = df, family = GLMMadaptive::censored.normal())\n\nData Descriptives:\nNumber of Observations: 2000\nNumber of Groups: 20 \n\nModel:\n family: censored normal\n link: identity \n\nFit statistics:\n   log.Lik      AIC      BIC\n -1196.614 2401.229 2405.212\n\nRandom effects covariance matrix:\n             StdDev\n(Intercept) 1.89308\n\nFixed effects:\n            Estimate Std.Err z-value p-value\n(Intercept)  -0.2812  0.4201 -0.6694 0.50326\nx             0.0742  0.0126  5.8995 &lt; 1e-04\n\nlog(residual std. dev.):\n  Estimate Std.Err\n   -0.6983  0.0192\n\nIntegration:\nmethod: adaptive Gauss-Hermite quadrature rule\nquadrature points: 11\n\nOptimization:\nmethod: hybrid EM and quasi-Newton\nconverged: TRUE \n\n\nWe can also use random slopes in GLMMadaptive. But only a single grouping factor (i.e., no nested (i.e. 2+ levels) or crossed random effects designs) is supported at the moment. {GLMMadaptive} is relatively fast. Unfortunately, when I had to fit more than 20 of these models for projects at least one of those models had convergence issues/errors. But perhaps someone with more feel for optimizer parameters is more successful.\nAnd contrary to {censReg} you can extract the random effects:\n\nGLMMadaptive::ranef(m) |&gt; head()\n\n  (Intercept)\n1   0.3581646\n2   1.2568000\n3   0.7057757\n4  -2.1809270\n5  -0.6329443\n6   3.2980978\n\n\n\n\n{brms} / {STAN}\nFor Bayesian software implementations the addition of random intercepts and slopes is less challenging, because it avoids the cumbersome integration. Similarly to {GLMMadaptive} we specify an additional indicator variable for our {brms} model, but this time left-censoring is indicated by \\(-1\\) (uncensored is \\(0\\) in brms and \\(1\\) indicates right censoring):\n\nm &lt;- brms::brm(y_obs | cens(censored) ~ 1 + x + (1|batch_id), \n               data = df |&gt; dplyr::mutate(censored = censored * -1), \n               seed = 2025, refresh = 1000,\n               backend = \"cmdstanr\", cores = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 27.7 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 28.6 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 30.8 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 31.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 29.7 seconds.\nTotal execution time: 32.0 seconds.\n\n\n\nsummary(m)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y_obs | cens(censored) ~ 1 + x + (1 | batch_id) \n   Data: dplyr::mutate(df, censored = censored * -1) (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~batch_id (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     2.02      0.33     1.48     2.78 1.01      513      776\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.27      0.43    -1.19     0.54 1.01      408      511\nx             0.07      0.01     0.05     0.10 1.00     1719     2001\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.01     0.48     0.52 1.00     1585     1527\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf we print the actual STAN code we see familiar elements from our earlier likelihood:\n\nbrms::stancode(m)\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0.1, 2.5);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept + Xc * b;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];\n    }\n    // vectorized log-likelihood contributions of censored data\n    target += normal_lpdf(Y[Jevent[1:Nevent]] | mu[Jevent[1:Nevent]], sigma);\n    target += normal_lccdf(Y[Jrcens[1:Nrcens]] | mu[Jrcens[1:Nrcens]], sigma);\n    target += normal_lcdf(Y[Jlcens[1:Nlcens]] | mu[Jlcens[1:Nlcens]], sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\n\nWith {brms} the possibilities are almost endless7 and fortunately its speed has also improved the last few years. My censored models usually converge out of the box, even with multiple random intercept terms. And with some reasonably strong priors I’ve also had success with high proportions of censored data (say &gt;80%).\n\n\n{MCMCglmm}\n{MCMCglmm} also supports left-censored multilevel (&gt;two-level?) models. The random effect notation is a bit different though. The intercept is varying by subject here (random intercept), random = ~us(1+fixed_effect):cluster gives a random intercept/slope model with estimated covariance, and random = ~idh(1+fixed_effect):cluster is the same but with the covariance set to 0. We can specify a left-censored outcome by creating two variables. If left-censored, y_obs_min is -Inf and y_obs_max takes on the value of the censoring point, in the other, non censored cases y_obs_min and y_obs_max are identical and take on the observed value:\n\nlibrary(MCMCglmm)\n\nm &lt;- MCMCglmm::MCMCglmm(cbind(y_obs_min, y_obs_max) ~ 1 + x , \n                        random = ~ batch_id, \n                        family = \"cengaussian\", \n                        data = df |&gt; \n                          dplyr::mutate(y_obs_min = ifelse(censored == 1, -Inf, y_obs),\n                                        y_obs_max = ifelse(censored == 1, censor_point, y_obs)) |&gt;\n                          data.frame(),\n                        nitt = 20000, thin = 1, burnin = 10000,\n                        verbose = FALSE)\n\n\nsummary(m)\n\n\n Iterations = 10001:20000\n Thinning interval  = 1\n Sample size  = 10000 \n\n DIC: 2077.777 \n\n G-structure:  ~batch_id\n\n         post.mean l-95% CI u-95% CI eff.samp\nbatch_id     4.123    1.728    7.005     3863\n\n R-structure:  ~units\n\n      post.mean l-95% CI u-95% CI eff.samp\nunits    0.2479   0.2298   0.2675     2027\n\n Location effects: cbind(y_obs_min, y_obs_max) ~ 1 + x \n\n            post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n(Intercept)  -0.27949 -1.11097  0.68806     9673  0.518    \nx             0.07432  0.04962  0.09862     4620 &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe G structure refers to the random effect structure, while the R structure is the residual structure.\nI think by default you do not get easy interpretable scale parameters, but you can obtain an estimate by taking the square root of the posterior distribution of the (co)variance matrices:\n\nsummary(sqrt(m$VCV))[[1]]\n\n              Mean          SD     Naive SE Time-series SE\nbatch_id 2.0001114 0.350291352 3.502914e-03   0.0057400762\nunits    0.4978472 0.009719887 9.719887e-05   0.0002157377\n\n\n\n\n{INLA}\nLastly I want to mention the {INLA} package. It seems like it’s neither purely frequentist nor fully Bayesian but instead approximates the posterior marginals. It’s mostly used in spatial statistics so perhaps this makes it the ideal candidate for censored concentration measurements with a spatial element, say a measurement campaign. Its interface is slightly unusual for the R regression packages I’m familiar with, but it is very, very fast! Many censored likelihood families are supported, but unfortunately the Gaussian family is not one of them:\n\nnames(INLA::inla.models()$likelihood) |&gt; \n    purrr::keep(~stringr::str_detect(.x, \"surv\"))\n\n[1] \"gammasurv\"        \"gammajwsurv\"      \"qloglogisticsurv\" \"lognormalsurv\"   \n[5] \"exponentialsurv\"  \"weibullsurv\"      \"loglogisticsurv\"  \"fmrisurv\"        \n[9] \"gompertzsurv\"    \n\n\nA censored gamma and lognormal family8 are present though and these are also useful in environmental studies.\nYou specify censoring using a inla.surv where left-censoring is coded as \\(2\\) and uncensored observations get a \\(1\\) indicator:\n\nlibrary(INLA)\n\n# simulate lognormal data\nsamples_p_batch &lt;- 100\nnr_batches &lt;- n/samples_p_batch\nsd_batch &lt;- 2\n\nre_batch &lt;- rep(rnorm(nr_batches, 0, sd_batch), each = samples_p_batch)\ny &lt;-  rlnorm(n, x * b_1 + re_batch, sd)\n\ncensor_point &lt;- quantile(y, prop_censored)\ncensored &lt;- y &lt;= censor_point\ny_obs &lt;- ifelse(censored, censor_point, y)\n\ndf &lt;- tibble::tibble(y, y_obs, x, censor_point, \n                     index = rep(1:samples_p_batch, times = nr_batches),\n                     batch_id = rep(1:nr_batches, each = samples_p_batch)) |&gt; \n  dplyr::mutate(censored = ifelse(censored == 1, 2, 1))\n\nsurv_obj &lt;- inla.surv(df$y_obs, df$censored)\nm &lt;- inla(surv_obj ~ 1 + x + f(batch_id, model = \"iid\"), \n          data = df, \n          family = \"lognormal.surv\", control.compute = list(config = TRUE))\n\n\nsummary(m)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.419, Running = 0.403, Post = 0.111, Total = 0.933 \nFixed effects:\n             mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n(Intercept) 0.374 0.493     -0.613    0.379      1.335 0.378   0\nx           0.098 0.013      0.073    0.098      0.123 0.098   0\n\nRandom effects:\n  Name    Model\n    batch_id IID model\n\nModel hyperparameters:\n                                              mean    sd 0.025quant 0.5quant\nPrecision for the lognormalsurv observations 3.985 0.153      3.679    3.987\nPrecision for batch_id                       0.224 0.074      0.105    0.215\n                                             0.975quant  mode\nPrecision for the lognormalsurv observations      4.280 4.001\nPrecision for batch_id                            0.392 0.198\n\nMarginal log-Likelihood:  -3353.37 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nThe random effects estimates are expressed on the precision scale but you can do the following to obtain the variance of the random effects on a familiar scale:9\n\nv1 &lt;- m |&gt; purrr::pluck(\"internal.marginals.hyperpar\", \n                        \"Log precision for batch_id\")\nv2 &lt;- m |&gt; purrr::pluck(\"internal.marginals.hyperpar\", \n                        \"Log precision for the lognormalsurv observations\")\n\nv1p &lt;- inla.tmarginal(function(x) 1/exp(x), v1) \nv2p &lt;- inla.tmarginal(function(x) 1/exp(x), v2)\n\nr &lt;- dplyr::bind_rows(\n  inla.zmarginal(v1p) |&gt; tibble::as_tibble() |&gt; dplyr::mutate(variable = \"batch_id\"),\n  inla.zmarginal(v2p) |&gt; tibble::as_tibble() |&gt; dplyr::mutate(variable = \"residual\"))\n\n\n r |&gt; \n  dplyr::mutate(mean = sqrt(mean))\n\n# A tibble: 2 × 8\n   mean      sd quant0.025 quant0.25 quant0.5 quant0.75 quant0.975 variable\n  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n1 2.23  1.77         2.56      3.73     4.63      5.87       9.41  batch_id\n2 0.501 0.00965      0.234     0.245    0.251     0.258      0.272 residual\n\n\nYou can also access the posterior through INLA::inla.posterior.sample.\nCompared to {brms}, {INLA} needs much stronger priors on random intercepts in a censored (lognormal) model in my experience. I have yet to encounter a real-life scenario where it could reliably do with a noninformative prior. Perhaps this is simply a consequence of the lognormal family or the combination of the lognormal family and some parameter space definition difference I’m not aware of, but I often have had to specify priors like these\n\nhalfcauchy  = \"expression:\n  sigma = exp(-theta/2);\n  gamma = 0.1;\n  log_dens = log(2) - log(pi) - log(gamma);\n  log_dens = log_dens - log(1 + (sigma / gamma)^2);\n  log_dens = log_dens - log(2) - theta / 2;\n  return(log_dens);\n\"\nhcpriorlist = list(prec = list(prior = halfcauchy))\n\nm &lt;- inla(surv_obj ~ 1 + f(batch_id, model = \"iid\", hyper = hcpriorlist), \n          data = df, family = \"lognormal.surv\", \n          control.compute = list(config = TRUE))\n\nfor good frequentist coverage properties – with less strong priors there would be huge outliers where some simulations had an estimated mean a couple magnitudes higher."
  },
  {
    "objectID": "posts/left-censored-models-showcase/index.html#other-packages",
    "href": "posts/left-censored-models-showcase/index.html#other-packages",
    "title": "A guide to fitting left-censored models with R",
    "section": "Other packages",
    "text": "Other packages\nThere are other packages available to fit these kinds of models that I have not covered here: {nlmixr2}, {ARpLMEC}, {lme4cens}, {gamlss} (supposedly also multilevel?), and {lmec} but they are either minimally used and validated10, or were somewhat clunky to use for me. There’s also talks to support censored regression models in {glmmTMB}. And there’s the {AER} package, but it seems like that is just a wrapper for the {survival} library with less functionality (i.e. no varying censoring points)."
  },
  {
    "objectID": "posts/left-censored-models-showcase/index.html#footnotes",
    "href": "posts/left-censored-models-showcase/index.html#footnotes",
    "title": "A guide to fitting left-censored models with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot to be confused with left truncation. In both truncation and censoring no information on the value of the observation beyond the truncation/censoring point is available. However, with censoring we know the number of observations beyond this point contrary to truncation where we do not observe any of the dependent variable or covariates – the entry is simply missing from your data table. See for example Senn, Holford, and Hockey (2012) and Andrew Gelman et al. (2013), p.224 for a more in depth explanation on the statistical connections between truncation and censoring.↩︎\nWhile I have not encountered it myself, I have read that (some) mass spectrometry instruments can also reach saturation. This will not be the topic of this post. I think though that saturation would introduce an additional right censoring mechanism or some type of non-linear effect – again not sure how this gets handled in practice.↩︎\nIn this post we assume the limit of detection is known, but this need not always be the case. See for example Andrew Gelman et al. (2013), p.226 on how to handle this.↩︎\nThe classical tobit model from economics assumes a normal distribution for outcome variable with left-censoring at 0.↩︎\nVia A. Gelman and Hill (2006) and Michael Clark his post.↩︎\nThough there seem to be some questions on if {censReg} is doing it correctly (per the author of the {GLMMadaptive} package).↩︎\nUnfortunately no censored predictors yet.↩︎\nNot entirely sure if the random effects also follow a lognormal distribution here. My simulations (not shown) suggest a lognormal normal distribution so this would be different from log transforming the outcome and using our {brms} model.↩︎\nSee for example [https://becarioprecario.bitbucket.io/inla-gitbook/ch-survival.html] and [https://www.paulamoraga.com/book-geospatial/sec-inla.html] for more details on how to work with {INLA}.↩︎\nWe are probably overconfident in statistical software and do not evaluate them sufficiently before using them in our work. See this post from Alex Hayes for some thoughts on checking statistical software.↩︎"
  }
]