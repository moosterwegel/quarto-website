{
  "hash": "69e2c2990f85e0a879dd78a5b99d0e92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A guide to fitting left-censored models with R\"\ndate: \"2025-04-21\"\nformat: \n  html: \n      echo: true\n      message: false\n      warning: false\n      code-fold: false\nexecute: \n  cache: true\ncitation: true\nbibliography: references.bib\n---\nIn environmental epidemiology -- and environmental studies more generally -- we often aim to detect trace levels of pollutants, metabolites, or biomarkers. Be it in a biospecimen like blood and urine or in environmental media such as soil, water, and air. This can be done with [liquid/gas chromatography mass spectrometry instruments](https://en.wikipedia.org/wiki/Gas_chromatography%E2%80%93mass_spectrometry) among others. Even though these instruments are very sensitive these days, they will sometimes hit their detection limits. This blog posts will go briefly go over on how to handle such data statistically speaking, and what options are available in `R` to fit these models. \n\n# Censoring\nEvery epidemiologist is familiar with the standard survival analysis example that describes right censored observations. In this example, some participants are lost to follow-up or do not experience the event during the study. As a result, for these participants we only know that a possible event occurred somewhere after our observation period. These observations are then said to be right censored at the end of this window.\n\nWith the aforementioned instruments, the situation is flipped: we only know a value is _less_ than a certain value. In these cases, our instrument can't discern the analyte from background noise below a certain point. In other words, measurements on such an instrument were limited to a minimum measurement of $l$ so any measurement smaller than $l$ was recorded as $l$. We refer to this minimum measurement as the limit of detection or limit of quantification and the data is said to be left-censored at this limit[^1]. Measurements smaller than this simply cannot be measured with this instrument.[^2]\n\n[^1]: Not to be confused with left truncation. In both truncation and censoring no information on the value of the observation beyond the truncation/censoring point is available. However, with censoring we know the number of observations beyond this point contrary to truncation where we do not observe any of the dependent variable or covariates -- the entry is simply missing from your data table. See for example @senn2012ghosts and @gelman2013bayesian, p.224 for a more in depth explanation on the statistical connections between truncation and censoring.\n\n[^2]: While I have not encountered it myself, I have read that (some) mass spectrometry instruments can also reach saturation. This will not be the topic of this post. I think though that saturation would introduce an additional right censoring mechanism or some type of non-linear effect -- again not sure how this gets handled in practice.\n\nWriting this in a more formal way, we can specify a latent regression model with an outcome that is either observed or unobserved:\n\n$$\ny_i^*= \\begin{cases}y_i & \\text { if } y_i>l \\\\\nl & \\text { if } y_i \\leq l \\end{cases}\n$$\nwhere $y_i$ represents the 'true', latent values, $y_i^*$ are values we observe in reality, and $l$ is the limit of detection[^3].\n\n[^3]: In this post we assume the limit of detection is known, but this need not always be the case. See for example @gelman2013bayesian, p.226 on how to handle this.\n\nIn most environmental studies (and the classical tobit model from economics)[^4] we then assume a normally distributed error term for the true values $y_i$ so we write \n\n[^4]: The classical tobit model from economics [assumes a normal distribution for outcome variable with left-censoring at 0](https://rdrr.io/cran/AER/man/tobit.html).\n\n$$\ny_i \\sim \\mathrm{~N}\\left(a+b x_i, \\sigma^2\\right), \\text { for } i=1, \\ldots, n\n$$\nWe can then estimate the parameters of the underlying distribution using the _observed_ data $y_i^*$ by including a separate term for the censoring mechanism in the likelihood:\n\n$$\n\\operatorname{Pr}\\left(y_i \\leq l\\right)=\\int_{-\\infty}^{l} \\mathrm{N}\\left(y_i \\mid a+b x_i, \\sigma^2\\right)=  \\Phi\\left(\\frac{l - (a+b x_i)}{\\sigma}\\right)\n$$\nsuch that the two terms of the likelihood become\n$$\ny_i^*= \\begin{cases}\\mathrm{N}\\left(y_i \\mid a+b x_i, \\sigma^2\\right) & \\text { if } y_i>l \\\\\n\\Phi\\left(\\frac{l - (a+b x_i)}{\\sigma}\\right) & \\text { if } y_i \\leq l \\end{cases}\n$$\nwith the likelihood for the uncensored data points just being the normal probability density function.\n\nVisually, the densities of $y$ and $y^*$ looks as follows:\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 100000\nprop_censored <- 0.3\ny <- rnorm(n)\ncensor_point <- quantile(y, prop_censored)\ncensored <- y <= censor_point\ny_obs <- ifelse(censored, censor_point, y)\ny_dens <- density(y, n = n)\ny_obs_dens <- density(y_obs, n = n)\n\nlibrary(ggplot2)\ndf_y_dens <- tibble::tibble(dens_x = y_dens$x,\n                            dens_y = y_dens$y,\n                            censor_point = censor_point,\n                            type = \"true\")\n\ndf_y_dens_obs <- tibble::tibble(dens_x = y_obs_dens$x,\n                                dens_y = y_obs_dens$y,\n                                censor_point = censor_point,\n                                type = \"observed\")\n\ntheme_set(theme_minimal() +\n            theme(axis.line = element_line(colour = \"black\", linewidth = rel(1)),\n                  panel.border = element_blank()))\n\nggplot(mapping = aes(x = dens_x, y = dens_y)) +\n  geom_line(data = df_y_dens, linewidth = 0.9,\n            aes(color = \"True values\"), linetype = \"dotted\") +\n  geom_line(data = subset(df_y_dens_obs, dens_x > censor_point), \n            linewidth = 0.9,\n            aes(color = \"Observed values\")) +\n  geom_vline(aes(xintercept = censor_point, color = \"Censor point\"), \n             linetype = \"dashed\", linewidth = 1) +\n  geom_area(data = subset(df_y_dens, dens_x <= censor_point),\n            fill = \"#822D2F\",\n            aes(color = \"Censored values\")) +  \n  labs(x = \"Density\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"True values\" = \"black\", \n                                \"Observed values\" = \"#375A6D\", \n                                \"Censor point\" = \"black\", \n                                \"Censored values\" = \"#822D2F\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(),  \n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = c(0.1, 0.8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png)\n:::\n:::\n\nso informally speaking, by virtue of the data being censored and not truncated we know the height of the probability mass and when we combine this information with the information from the uncensored observations, and the assumption we made on its distributional form we can 'spread out' the probability mass across the censored region and estimate the parameters of the latent/true data structure.\n\nIf we additionally simulate an effect of $x$ on $y$ and plot $x$ vs $y$ we can easily see that ignoring the censoring mechanism (or substituting by a constant value below the limit of detection as is often done -- not shown) biases the estimate of the slope. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 2000\nprop_censored <- 0.3\nb_1 <- 0.1\nsd <- 0.5\n\nx <- rnorm(n)\ny <-  rnorm(n, x * b_1, sd)\ncensor_point <- quantile(y, prop_censored)\ncensored <- y <= censor_point\ny_obs <- ifelse(censored, censor_point, y)\n\ndf <- tibble::tibble(y, y_obs, x, censor_point)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf |> \n  ggplot() + \n  geom_point(aes(x = x, y = y_obs), color = '#375A6D') +\n  geom_point(data = subset(df, y <= censor_point),\n             aes(x = x, y = y), color = '#822D2F') +\n  geom_smooth(aes(x = x, y = y_obs, color = \"Observed values fit\"), \n              method = \"lm\", linetype = \"solid\") +\n  geom_smooth(aes(x = x, y = y, color = \"True values fit\"), \n              method = 'lm', linetype = \"dotted\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"True values fit\" = \"dotted\", \"Observed values fit\" = \"solid\")) +\n  scale_color_manual(name = \"\",\n                     values = c(\"True values fit\" = \"black\", \"Observed values fit\" = \"black\")) +\n  labs(x = expression(x),\n       y = expression(y)) +\n  theme(legend.position = c(0.15, 0.9))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png)\n:::\n:::\n\n\n# Fitting a left-censored model using R\nWe can fit a model in `R` that accounts for the censoring mechanism by writing down the likelihood and passing it to `optim` to find the maximum:[^5]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_lik <- function(parameter_vector, x, y, l) {\n  a <- parameter_vector[1]\n  b <- parameter_vector[2]\n  sigma <- parameter_vector[3]\n  \n  linear_predictor <- a + b*x\n  \n  # you could also do this more explicitly with an indicator variable and a multiplication \n  ll_vec <- ifelse(y > l, \n                   dnorm(y, linear_predictor, sigma, log = TRUE),\n                   pnorm((l - linear_predictor)/sigma, log = TRUE))\n  \n  return(sum(ll_vec))\n}\n\n# initialize with a fit from the observed data, inits <- runif(3) will also work for simple models\ninitmod <- lm(y_obs ~ 1 + x, data = df)\nprint(c(coef(initmod), sigma = summary(initmod)$sigma))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x       sigma \n 0.09927942  0.06638911  0.38652839 \n```\n\n\n:::\n\n```{.r .cell-code}\ninit <- c(coef(initmod), sigma = log(summary(initmod)$sigma))\n\nmle <- optim(init, log_lik, lower = c(-Inf,-Inf, 1.e-5),\n             method = \"L-BFGS-B\", control = list(fnscale = -1), \n             x = df[['x']], y = df[['y_obs']], l = censor_point)\nmle$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)            x        sigma \n0.0005055924 0.0974616424 0.5137657833 \n```\n\n\n:::\n:::\n\nWe can see that the procedure managed to recover the true values for $\\sigma$ (= 0.5) and $\\beta_1$ (= 0.1). \n\n[^5]: Via @gelman2006data and [Michael Clark](https://m-clark.github.io/models-by-example/tobit.html) his post.\n\nOf course there are also `R` packages that can do such a procedure for us. Both frequentist and Bayesian. Below an overview:\n\n### {survival}\nIn {survival} you can specify left-censored models just as you would specify right censored models using the `Surv` object. The `censor_point` can also be a vector and thereby allow your observations to be censored at different levels/limits.\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- survival::survreg(survival::Surv(y_obs,\n                                      y_obs > censor_point,\n                                      type = \"left\") ~ 1 + x,\n                       data = df,\n                       dist = \"gaussian\")\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsurvival::survreg(formula = survival::Surv(y_obs, y_obs > censor_point, \n    type = \"left\") ~ 1 + x, data = df, dist = \"gaussian\")\n                Value Std. Error      z      p\n(Intercept)  0.000506   0.012272   0.04   0.97\nx            0.097462   0.011836   8.23 <2e-16\nLog(scale)  -0.665992   0.020230 -32.92 <2e-16\n\nScale= 0.514 \n\nGaussian distribution\nLoglik(model)= -1587.6   Loglik(intercept only)= -1621.4\n\tChisq= 67.57 on 1 degrees of freedom, p= 2e-16 \nNumber of Newton-Raphson Iterations: 3 \nn= 2000 \n```\n\n\n:::\n:::\n\nYou can get the same result by specifying the left-censoring as a special case of interval censoring:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- survival::survreg(survival::Surv(time = ifelse(y_obs == censor_point, -999, y_obs), \n                                      time2 = censor_point,\n                                      event = ifelse(y_obs == censor_point, 3, 1), \n                                      type = \"interval\") ~ 1 + x, \n                       data = df,\n                       dist = \"gaussian\")\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsurvival::survreg(formula = survival::Surv(time = ifelse(y_obs == \n    censor_point, -999, y_obs), time2 = censor_point, event = ifelse(y_obs == \n    censor_point, 3, 1), type = \"interval\") ~ 1 + x, data = df, \n    dist = \"gaussian\")\n                Value Std. Error      z      p\n(Intercept)  0.000506   0.012272   0.04   0.97\nx            0.097462   0.011836   8.23 <2e-16\nLog(scale)  -0.665992   0.020230 -32.92 <2e-16\n\nScale= 0.514 \n\nGaussian distribution\nLoglik(model)= -1587.6   Loglik(intercept only)= -1621.4\n\tChisq= 67.57 on 1 degrees of freedom, p= 2e-16 \nNumber of Newton-Raphson Iterations: 6 \nn= 2000 \n```\n\n\n:::\n:::\n\nThere's also a lognormal distribution available in {survival}. \n\n### {censReg}\n{censReg} does not support varying censoring points, as the `left` argument only allows a constant, whereas {survival} allows this by specifying a `censor_point` vector. \n\n::: {.cell}\n\n```{.r .cell-code}\nm <- censReg::censReg(y_obs ~ 1 + x, \n                      data = df, \n                      method = \"BHHH\", \n                      left = min(df$y_obs), right = Inf)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ncensReg::censReg(formula = y_obs ~ 1 + x, left = min(df$y_obs), \n    right = Inf, data = df, method = \"BHHH\")\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n          2000            600           1400              0 \n\nCoefficients:\n              Estimate Std. error t value  Pr(> t)    \n(Intercept)  0.0005067  0.0122142   0.041    0.967    \nx            0.0974645  0.0119876   8.130 4.28e-16 ***\nlogSigma    -0.6659855  0.0209399 -31.805  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBHHH maximisation, 17 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-likelihood: -1587.58 on 3 Df\n```\n\n\n:::\n:::\n\n\n## Multilevel left-censored models\nSometimes your data is more complicated and you want to specify different levels of your data. For example: a random intercept that specifies what subject or batch an observation belongs to. Your `R` package options are more limited then, as the frequentist approach now needs to additionally integrate over the random effects in the definition of the marginal likelihood. \n\nThe {survival} model does not support such models while {censReg} supports only a simple two-level random intercept model:\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_p_batch <- 100\nnr_batches <- n/samples_p_batch\nsd_batch <- 2\n\nre_batch <- rep(rnorm(nr_batches, 0, sd_batch), each = samples_p_batch)\ny <-  rnorm(n, x * b_1 + re_batch, sd)\n\ncensor_point <- quantile(y, prop_censored)\ncensored <- y <= censor_point\ny_obs <- ifelse(censored, censor_point, y)\n\ndf <- tibble::tibble(y, y_obs, x, censor_point, \n                     index = rep(1:samples_p_batch, times = nr_batches),\n                     batch_id = rep(1:nr_batches, each = samples_p_batch))\n```\n:::\n\n\n### {censReg}\nTo get such a two-level random intercept model to work in {censReg} we have to create a {plm} object first[^6]\n\n[^6]: Though there seem to be some questions on if {censReg} is doing it [correctly](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2021q3/029715.html) (per the author of the {GLMMadaptive} package). \n\n::: {.cell}\n\n```{.r .cell-code}\npData <- plm::pdata.frame(df, c( \"batch_id\", \"index\"))\n\nm <- censReg::censReg(y_obs ~ 1 + x, \n                      data = pData, \n                      method = \"BHHH\", \n                      left = min(df$y_obs), right = Inf)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ncensReg::censReg(formula = y_obs ~ 1 + x, left = min(df$y_obs), \n    right = Inf, data = pData, method = \"BHHH\")\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n          2000            600           1400              0 \n\nCoefficients:\n             Estimate Std. error t value  Pr(> t)    \n(Intercept) -0.091705   0.004090 -22.423  < 2e-16 ***\nx            0.083038   0.016855   4.927 8.37e-07 ***\nlogSigmaMu   0.454964   0.002405 189.172  < 2e-16 ***\nlogSigmaNu  -0.474995   0.007687 -61.794  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBHHH maximisation, 150 iterations\nReturn code 4: Iteration limit exceeded (iterlim)\nLog-likelihood: -1475.026 on 4 Df\n```\n\n\n:::\n:::\n\n\n### {GLMMadaptive}\nOne other frequentist package available for your left-censored models is {GLMMadaptive}. In addition to your observed outcome, you need to specify a vector with an indicator variable that describes whether the observation was censored:\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- GLMMadaptive::mixed_model(cbind(y_obs, censored) ~ 1 + x, \n                               random = ~ 1|batch_id, \n                               data = df,\n                               family = GLMMadaptive::censored.normal())\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nGLMMadaptive::mixed_model(fixed = cbind(y_obs, censored) ~ 1 + \n    x, random = ~1 | batch_id, data = df, family = GLMMadaptive::censored.normal())\n\nData Descriptives:\nNumber of Observations: 2000\nNumber of Groups: 20 \n\nModel:\n family: censored normal\n link: identity \n\nFit statistics:\n   log.Lik      AIC      BIC\n -1196.614 2401.229 2405.212\n\nRandom effects covariance matrix:\n             StdDev\n(Intercept) 1.89308\n\nFixed effects:\n            Estimate Std.Err z-value p-value\n(Intercept)  -0.2812  0.4201 -0.6694 0.50326\nx             0.0742  0.0126  5.8995 < 1e-04\n\nlog(residual std. dev.):\n  Estimate Std.Err\n   -0.6983  0.0192\n\nIntegration:\nmethod: adaptive Gauss-Hermite quadrature rule\nquadrature points: 11\n\nOptimization:\nmethod: hybrid EM and quasi-Newton\nconverged: TRUE \n```\n\n\n:::\n:::\n\nWe can also use random slopes in GLMMadaptive. But only a single grouping factor (i.e., no nested (i.e. 2+ levels) or crossed random effects designs) is [supported](https://cran.r-project.org/web/packages/GLMMadaptive/vignettes/GLMMadaptive.html) at the moment. {GLMMadaptive} is relatively fast. Unfortunately, when I had to fit more than 20 of these models for projects at least one of those models had convergence issues/errors. But perhaps someone with more feel for optimizer parameters is more successful.\n\nAnd contrary to {censReg} you can extract the random effects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGLMMadaptive::ranef(m) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)\n1   0.3581646\n2   1.2568000\n3   0.7057757\n4  -2.1809270\n5  -0.6329443\n6   3.2980978\n```\n\n\n:::\n:::\n\n\n\n### {brms} / {STAN}\nFor Bayesian software implementations the addition of random intercepts and slopes is less challenging, because it avoids the cumbersome integration. Similarly to {GLMMadaptive} we specify an additional indicator variable for our {brms} model, but this time left-censoring is indicated by $-1$ (uncensored is $0$ in `brms` and $1$ indicates right censoring):\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- brms::brm(y_obs | cens(censored) ~ 1 + x + (1|batch_id), \n               data = df |> dplyr::mutate(censored = censored * -1), \n               seed = 2025, refresh = 1000,\n               backend = \"cmdstanr\", cores = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 27.7 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 28.6 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 30.8 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 31.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 29.7 seconds.\nTotal execution time: 32.0 seconds.\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y_obs | cens(censored) ~ 1 + x + (1 | batch_id) \n   Data: dplyr::mutate(df, censored = censored * -1) (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~batch_id (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     2.02      0.33     1.48     2.78 1.01      513      776\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.27      0.43    -1.19     0.54 1.01      408      511\nx             0.07      0.01     0.05     0.10 1.00     1719     2001\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.01     0.48     0.52 1.00     1585     1527\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\nIf we print the actual STAN code we see familiar elements from our earlier likelihood:\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms::stancode(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int<lower=-1,upper=2> cens;\n  int<lower=1> K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int<lower=1> Kc;  // number of population-level effects after centering\n  // data for group-level effects of ID 1\n  int<lower=1> N_1;  // number of grouping levels\n  int<lower=1> M_1;  // number of coefficients per level\n  array[N] int<lower=1> J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real<lower=0> sigma;  // dispersion parameter\n  vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0.1, 2.5);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept + Xc * b;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];\n    }\n    // vectorized log-likelihood contributions of censored data\n    target += normal_lpdf(Y[Jevent[1:Nevent]] | mu[Jevent[1:Nevent]], sigma);\n    target += normal_lccdf(Y[Jrcens[1:Nrcens]] | mu[Jrcens[1:Nrcens]], sigma);\n    target += normal_lcdf(Y[Jlcens[1:Nlcens]] | mu[Jlcens[1:Nlcens]], sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n```\n\n\n:::\n:::\n\n\nWith {brms} the possibilities are almost endless[^7] and fortunately its speed has also improved the last few years. My censored models usually converge out of the box, even with multiple random intercept terms. And with some reasonably strong priors I've also had success with high proportions of censored data (say >80%).\n\n[^7]: Unfortunately [no censored predictors yet](https://github.com/paul-buerkner/brms/issues/565).\n\n### {MCMCglmm}\n{MCMCglmm} also supports left-censored multilevel (>two-level?) models. The random effect notation is a bit [different](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q1/019896.html) though. The intercept is varying by subject here (random intercept), `random = ~us(1+fixed_effect):cluster` gives a random intercept/slope model with estimated covariance, and `random = ~idh(1+fixed_effect):cluster` is the same but with the covariance set to 0. We can specify a left-censored outcome by creating two variables. If left-censored, `y_obs_min` is `-Inf` and `y_obs_max` takes on the value of the censoring point, in the other, non censored cases `y_obs_min` and `y_obs_max` are identical and take on the observed value:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MCMCglmm)\n\nm <- MCMCglmm::MCMCglmm(cbind(y_obs_min, y_obs_max) ~ 1 + x , \n                        random = ~ batch_id, \n                        family = \"cengaussian\", \n                        data = df |> \n                          dplyr::mutate(y_obs_min = ifelse(censored == 1, -Inf, y_obs),\n                                        y_obs_max = ifelse(censored == 1, censor_point, y_obs)) |>\n                          data.frame(),\n                        nitt = 20000, thin = 1, burnin = 10000,\n                        verbose = FALSE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Iterations = 10001:20000\n Thinning interval  = 1\n Sample size  = 10000 \n\n DIC: 2077.777 \n\n G-structure:  ~batch_id\n\n         post.mean l-95% CI u-95% CI eff.samp\nbatch_id     4.123    1.728    7.005     3863\n\n R-structure:  ~units\n\n      post.mean l-95% CI u-95% CI eff.samp\nunits    0.2479   0.2298   0.2675     2027\n\n Location effects: cbind(y_obs_min, y_obs_max) ~ 1 + x \n\n            post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n(Intercept)  -0.27949 -1.11097  0.68806     9673  0.518    \nx             0.07432  0.04962  0.09862     4620 <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nThe G structure [refers](https://stats.stackexchange.com/questions/32994/what-are-r-structure-g-structure-in-a-glmm) to the random effect structure, while the R structure is the residual structure.\n\nI think by default you do not get easy interpretable scale parameters, but you can [obtain](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020390.html) an estimate by taking the square root of the posterior distribution of the (co)variance matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sqrt(m$VCV))[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Mean          SD     Naive SE Time-series SE\nbatch_id 2.0001114 0.350291352 3.502914e-03   0.0057400762\nunits    0.4978472 0.009719887 9.719887e-05   0.0002157377\n```\n\n\n:::\n:::\n\n\n### {INLA}\nLastly I want to mention the {INLA} package. It seems like it's neither purely frequentist nor fully Bayesian but instead _approximates_ the posterior marginals. It's mostly used in spatial statistics so perhaps this makes it the ideal candidate for censored concentration measurements with a spatial element, say a measurement campaign. Its interface is slightly unusual for the `R` regression packages I'm familiar with, but it is very, very fast! Many censored likelihood families are supported, but unfortunately the Gaussian family is not one of them:\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(INLA::inla.models()$likelihood) |> \n    purrr::keep(~stringr::str_detect(.x, \"surv\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"gammasurv\"        \"gammajwsurv\"      \"qloglogisticsurv\" \"lognormalsurv\"   \n[5] \"exponentialsurv\"  \"weibullsurv\"      \"loglogisticsurv\"  \"fmrisurv\"        \n[9] \"gompertzsurv\"    \n```\n\n\n:::\n:::\n\nA censored gamma and lognormal family[^8] are present though and these are also useful in environmental studies. \n\n[^8]: Not entirely sure if the random effects also follow a lognormal distribution here. My simulations (not shown) suggest a lognormal normal distribution so this would be different from log transforming the outcome and using our {brms} model. \n\nYou specify censoring using a `inla.surv` where left-censoring is coded as $2$ and uncensored observations get a $1$ indicator:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\n\n# simulate lognormal data\nsamples_p_batch <- 100\nnr_batches <- n/samples_p_batch\nsd_batch <- 2\n\nre_batch <- rep(rnorm(nr_batches, 0, sd_batch), each = samples_p_batch)\ny <-  rlnorm(n, x * b_1 + re_batch, sd)\n\ncensor_point <- quantile(y, prop_censored)\ncensored <- y <= censor_point\ny_obs <- ifelse(censored, censor_point, y)\n\ndf <- tibble::tibble(y, y_obs, x, censor_point, \n                     index = rep(1:samples_p_batch, times = nr_batches),\n                     batch_id = rep(1:nr_batches, each = samples_p_batch)) |> \n  dplyr::mutate(censored = ifelse(censored == 1, 2, 1))\n\nsurv_obj <- inla.surv(df$y_obs, df$censored)\nm <- inla(surv_obj ~ 1 + x + f(batch_id, model = \"iid\"), \n          data = df, \n          family = \"lognormal.surv\", control.compute = list(config = TRUE))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.419, Running = 0.403, Post = 0.111, Total = 0.933 \nFixed effects:\n             mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n(Intercept) 0.374 0.493     -0.613    0.379      1.335 0.378   0\nx           0.098 0.013      0.073    0.098      0.123 0.098   0\n\nRandom effects:\n  Name\t  Model\n    batch_id IID model\n\nModel hyperparameters:\n                                              mean    sd 0.025quant 0.5quant\nPrecision for the lognormalsurv observations 3.985 0.153      3.679    3.987\nPrecision for batch_id                       0.224 0.074      0.105    0.215\n                                             0.975quant  mode\nPrecision for the lognormalsurv observations      4.280 4.001\nPrecision for batch_id                            0.392 0.198\n\nMarginal log-Likelihood:  -3353.37 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\nThe random effects estimates are expressed on the precision scale but you can do the following to obtain the variance of the random effects on a familiar scale:[^9]\n\n[^9]: See for example [https://becarioprecario.bitbucket.io/inla-gitbook/ch-survival.html] and [https://www.paulamoraga.com/book-geospatial/sec-inla.html] for more details on how to work with {INLA}.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv1 <- m |> purrr::pluck(\"internal.marginals.hyperpar\", \n                        \"Log precision for batch_id\")\nv2 <- m |> purrr::pluck(\"internal.marginals.hyperpar\", \n                        \"Log precision for the lognormalsurv observations\")\n\nv1p <- inla.tmarginal(function(x) 1/exp(x), v1) \nv2p <- inla.tmarginal(function(x) 1/exp(x), v2)\n\nr <- dplyr::bind_rows(\n  inla.zmarginal(v1p) |> tibble::as_tibble() |> dplyr::mutate(variable = \"batch_id\"),\n  inla.zmarginal(v2p) |> tibble::as_tibble() |> dplyr::mutate(variable = \"residual\"))\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n r |> \n  dplyr::mutate(mean = sqrt(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 8\n   mean      sd quant0.025 quant0.25 quant0.5 quant0.75 quant0.975 variable\n  <dbl>   <dbl>      <dbl>     <dbl>    <dbl>     <dbl>      <dbl> <chr>   \n1 2.23  1.77         2.56      3.73     4.63      5.87       9.41  batch_id\n2 0.501 0.00965      0.234     0.245    0.251     0.258      0.272 residual\n```\n\n\n:::\n:::\n\nYou can also access the posterior through `INLA::inla.posterior.sample`.\n\nCompared to {brms}, {INLA} needs much stronger priors on random intercepts in a censored (lognormal) model in my experience. I have yet to encounter a real-life scenario where it could reliably do with a noninformative prior. Perhaps this is simply a consequence of the lognormal family or the combination of the lognormal family and some parameter space definition difference I'm not aware of, but I often have had to specify priors like these\n\n::: {.cell}\n\n```{.r .cell-code}\nhalfcauchy  = \"expression:\n  sigma = exp(-theta/2);\n  gamma = 0.1;\n  log_dens = log(2) - log(pi) - log(gamma);\n  log_dens = log_dens - log(1 + (sigma / gamma)^2);\n  log_dens = log_dens - log(2) - theta / 2;\n  return(log_dens);\n\"\nhcpriorlist = list(prec = list(prior = halfcauchy))\n\nm <- inla(surv_obj ~ 1 + f(batch_id, model = \"iid\", hyper = hcpriorlist), \n          data = df, family = \"lognormal.surv\", \n          control.compute = list(config = TRUE))\n```\n:::\n\nfor good frequentist coverage properties -- with less strong priors there would be huge outliers where some simulations had an estimated mean a couple magnitudes higher.\n\n## Other packages\nThere are [other packages available](https://cran.r-project.org/web/views/MixedModels.html) to fit these kinds of models that I have not covered here: {nlmixr2}, {ARpLMEC}, {lme4cens}, {gamlss} (supposedly also multilevel?), and {lmec} but they are either minimally used and validated[^10], or were somewhat clunky to use for me. There's also talks to support censored regression models in [{glmmTMB}](https://github.com/glmmTMB/glmmTMB/issues/690). And there's the {AER} package, but it seems like that is just a [wrapper](https://m-clark.github.io/models-by-example/tobit.html) for the {survival} library with less functionality (i.e. no varying censoring points).\n\n[^10]: We are probably overconfident in statistical software and do not evaluate them sufficiently before using them in our work. See [this post from Alex Hayes](https://www.alexpghayes.com/post/2019-06-07_testing-statistical-software/) for some thoughts on checking statistical software.\n\n\n::: {.cell}\n\n:::\n\n\n---\nnocite: |\n  @gelman2006data, @gelman2021regression, @helsel_fabricating_2006, @helsel2005nondetects\n---",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}