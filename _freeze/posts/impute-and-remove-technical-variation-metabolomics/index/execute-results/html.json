{
  "hash": "946cc11c558303a4dc41fbecfb7622f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Imputing and removing technical variation in left censored metabolomic data\"\ndate: \"2025-04-28\"\nformat: \n  html: \n      echo: true\n      message: false\n      warning: false\n      code-fold: false\ncitation: true\nexecute: \n  cache: true\nbibliography: references.bib\n---\nThe field of environmental epidemiology has made significant investments in untargeted (also known as nontargeted)  metabolomics over the past few years, to gain deeper insights into exposure levels, biological mechanisms, and their relationship to disease.  These investments are starting to pay dividends by enabling larger studies, but this scale-up comes with its own challenges. With sample sizes growing substantially, processing all study samples in a single batch is no longer feasible, forcing analyses to span longer periods. These extended timeframes introduce greater variation in technical, non-biological factors that contaminate the data with unwanted variation. Additionally, as I've discussed in [a previous post](/posts/left-censored-models-showcase/), this metabolomic data is often left censored, creating further analytical challenges. In this post, I'll share a method I developed that handles both the imputation of nondetects and the removal of technical variation in one and the same model[^1].\n\n[^1]: If your research question pertains directly to the exposure or metabolite level itself -- rather than its relationship as a predictor for an outcome -- then imputation is not necessary of course, because you can directly use the fitted left censored model.\n\n# Technical variation in untargeted/nontargeted metabolomics\nTo illustrate the model, I will start with simulating some left censored data that contains batch and plate effects (an example of technical variation). In this model, the study samples are processed in (100-well) plates that are nested within the batches. A proportion of these values will be censored. The censoring point (limit of detection) will differ per batch:\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 2000\nsamples_p_batch <- 400\nsamples_p_plate <- 100\n\nnr_batches <- n/samples_p_batch\nnr_plates <- nr_batches * (samples_p_batch/samples_p_plate) \n\nmu <- 12\nsd_batch <- 1.5\nsd_plate <- 0.5\nsd <- 1\nb_1 <- 0.1\n\nset.seed(1234)\nx <- rnorm(n)\n\nre_batch <- rep(rnorm(nr_batches, 0, sd_batch), each = samples_p_batch)\nre_plate <- rep(rnorm(nr_plates, 0, sd_plate), each = samples_p_plate)\nre <- re_batch + re_plate\ny <-  rlnorm(n, mu + x * b_1 + re, sd)\n\ndf <- tibble::tibble(y, x, \n                     batch_id = as.factor(rep(1:nr_batches, each = samples_p_batch))) |> \n  dplyr::group_by(batch_id) |> \n  dplyr::mutate(lod = quantile(y, runif(1, 0.05, 0.5)),\n                censored = y <= lod,\n                y_obs = ifelse(censored, lod, y),\n                log_lod = log(lod),\n                log_y = log(y),\n                log_y_obs = log(y_obs),\n                plate_id = as.factor(rep(1:(samples_p_batch/samples_p_plate), \n                                         each = samples_p_plate))) |> \n  dplyr::ungroup() |> \n  dplyr::mutate(inj_order = dplyr::row_number()) |> \n  tidyr::unite(plate_nested_coding, c(batch_id, plate_id), remove = FALSE) \n```\n:::\n\n\n<!-- # ```{r} -->\n<!-- # df |>  -->\n<!-- #   ggplot(aes(inj_order, log_y)) +  -->\n<!-- #   see::scale_color_bluebrown() + -->\n<!-- #   geom_point(aes(color = batch_id)) +  -->\n<!-- #   labs(x = x_lab, y = y_lab) + -->\n<!-- #   theme_minimal() + -->\n<!-- #   theme(legend.position = 'top') -->\n<!-- # ``` -->\nThe plot below shows the technical variation (the different colors correspond to different batches) and the left censored nature of the (untargeted) metabolic data. The data is left censored because our instruments don't have unlimited sensitivity. Due to left censoring (dashed line), we observe only part of the distribution. Without the variation introduced by the different batches and plates the distribution would be much narrower. It's also interesting to see that you can have an apparent drift over time that's just the consequence of random variation between batches. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nx_lab <- \"Injection order\"\ny_lab <- \"y\"\n\nsub_1 <- 'Ideally the data looks like this (no censoring or batch-effects)'\nsub_2 <- 'Reality: there are batch effects'\nsub_3 <- \"Reality: there are batch effects + censoring due to detection limits\"\n\n# via https://github.com/easystats/see/blob/main/R/scale_color_bluebrown.R\nbluebrown_colors_list <- c(\n  lightblue = \"#6DC0E0\",\n  darkbrown = \"#391D07\",\n  blue = \"#5B93AE\",\n  lightbrown = \"#92673C\",\n  darkblue = \"#1F4454\",\n  brown = \"#61381A\"\n)\n\nlinewidth <- 1.1\nnr_bins <- 25\naxis_range_bins <- 420\n\ndf_plot <- df |> \n  dplyr::mutate(log_y_no_batch = log_y - re,\n                log_y_lab = ifelse(censored, NA, log_y_obs)) \n\nbatch_lod_hline <- function(df_plot, nr_batches, samples_p_batch, linewidth, ...) {\n  batches <- sort(unique(df_plot$batch_id))\n  \n  segments <- lapply(1:nr_batches, function(i) {\n    batch <- batches[i]\n    batch_data <- df_plot[df_plot$batch_id == batch, ]\n    lod_value <- unique(batch_data$log_lod)\n    \n    x_start <- (i-1) * samples_p_batch + 1\n    x_end <- i * samples_p_batch\n    \n    list(\n      geom_segment(aes(y = lod_value,  yend = lod_value, x = x_start, xend = x_end), \n                   linewidth = linewidth, ...),\n      geom_segment(aes(y = lod_value,  yend = lod_value, x = x_end, xend = max(df_plot$inj_order)), \n                   linewidth = linewidth - 0.5, ...)\n    )\n  })\n  return(do.call(list, unlist(segments, recursive = FALSE)))\n}\n\np01a <- df_plot |> \n  ggplot(aes(x = inj_order, y = log_y_no_batch, color = batch_id)) + \n  geom_point() + \n  ylim(c(min(df$log_y), max(df$log_y))) +\n  scale_color_manual(values = unname(bluebrown_colors_list)) +\n  theme_minimal() +\n  theme(legend.position = 'none') +\n  labs(x = x_lab, y = y_lab, subtitle = sub_1)\n\np02a <- df_plot |> \n  ggplot(aes(x = inj_order, y = log_y, color = batch_id)) + \n  geom_point() + \n  ylim(c(min(df$log_y), max(df$log_y))) +\n  scale_color_manual(values = unname(bluebrown_colors_list)) +\n  theme_minimal() +\n  theme(legend.position = 'none') +\n  labs(x = x_lab,  y = y_lab, subtitle = sub_2)\n\np03a <- df_plot |> \n  ggplot(aes(x = inj_order, y = log_y_lab, color = batch_id)) + \n  geom_point() + \n  geom_point(data = subset(df_plot, censored), \n             aes(x = inj_order, y = log_y), color = '#dbdbdb', shape = 5) +\n  batch_lod_hline(df_plot, nr_batches, samples_p_batch,\n                  linewidth = linewidth, linetype = 'dashed', color = 'black') +\n  ylim(c(min(df$log_y), max(df$log_y))) +\n  scale_color_manual(values = unname(bluebrown_colors_list)) +\n  theme_minimal() +\n  theme(legend.position = 'none') +\n  labs(x = x_lab, y = y_lab, subtitle = sub_3)\n\np01b <- df_plot |> \n  ggplot(aes(x = log_y_no_batch)) +\n  geom_histogram(bins = nr_bins, fill = 'black', alpha = 0.7, color = 'white') +\n  xlim(c(min(df$log_y), max(df$log_y))) +\n  ylim(c(0, axis_range_bins)) +\n  coord_flip() +\n  theme_void()\n\np02b <- df_plot |> \n  ggplot(aes(x = log_y)) +\n  geom_histogram(bins = nr_bins, fill = 'black', alpha = 0.7, color = 'white') +\n  xlim(c(min(df$log_y), max(df$log_y))) +\n  ylim(c(0, axis_range_bins)) +\n  coord_flip() +\n  theme_void()\n\np03b <- df_plot |> \n  ggplot(aes(x = log_y_lab)) +\n  geom_histogram(bins = nr_bins, fill = 'black', alpha = 0.7, color = 'white') +\n  geom_vline(aes(xintercept = log_lod), linetype = 'dashed', linewidth = linewidth - 0.5) +\n  xlim(c(min(df$log_y), max(df$log_y))) +\n  ylim(c(0, axis_range_bins)) +\n  coord_flip() +\n  theme_void()\n\nlibrary(patchwork)\np01 <- p01a + p01b + plot_layout(ncol = 2, nrow = 1, widths = c(3, 1)) \np02 <- p02a + p02b + plot_layout(ncol = 2, nrow = 1, widths = c(3, 1)) \np03 <- p03a + p03b + plot_layout(ncol = 2, nrow = 1, widths = c(3, 1))\n\np01 / p02 / p03 \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n# The status quo of imputation and technical variation adjustment \nAs I tried to illustrate using these plots, technical variation and left censoring due to limits of detection (LOD) can be significant hurdles in metabolomics analysis. Yet, read method sections in the literature, and you'll often find they're treated like an afterthought. If addressed at all, methods sections will often only briefly state: 'batch effects were removed using ComBat' (despite ComBat fundamentally ignoring the censored nature of data below the LOD) or 'data was imputed using methods X or Y' (referencing imputation techniques like those by Lubin et al. or Lazar et al. which focus solely on left censoring without inherently handling batch effects). \n\nFurthermore, these references to 'foundational' imputation methods (like those by Lubin et al. or Lazar et al.) typically lack specifics on the exact imputation model; they neither detail which variables were included in the current study's imputation nor do the original papers offer clear guidance on which should be included (because it wasn't the goal of those papers). However, this is important for the analyst and the reader to know, as imputation models and the models for the subsequent analysis need to be congenial -- meaning the imputation must account for the relationships you intend to study later. The variables included in imputation can matter greatly!\n\n# Why a unified model is better\nMoreover, I will argue that the imputation and technical variation adjustment is best done in one and the same model, because the technical variation and left censoring are often intertwined. Namely, the same technical factors causing batch-to-batch (or plate-to-plate) shifts in metabolite levels can also influence instrument sensitivity or baseline noise, thereby changing the effective LOD for different batches. This means that you cannot obtain an unbiased estimate of the technical variation without modeling the left censoring, or the left censoring (imputes) without modeling the the technical variation. \n\nIn other words, if you adjust for batch effects before imputation (e.g., using methods like ComBat that ignore censoring), you are applying adjustments to data that either contains arbitrary placeholders for non-detects (like LOD/2) or simply ignores the information that the true value is below a certain batch-specific threshold. This distorts the estimation of the batch effect itself. And the higher proportion of nondetects, the worse this bias can get. \n\nOne might also consider a seemingly pragmatic two-step approach: first, perform multiple imputation using a model that accounts for batch-specific LODs and includes batch and plate effects (perhaps as fixed effects[^2]); second, fit a separate mixed model to each imputed dataset to estimate and remove these batch effects. However, this sequential process introduces several statistical challenges compared to a unified model:\n* Treating batch effects as fixed during imputation (often a limitation of available software for censored models) but then as random during the adjustment step creates a conceptual mismatch in how technical variation is modeled and removed.\n* Estimating imputation parameters and batch effects sequentially can be less statistically efficient than a unified model where all parameters are estimated simultaneously, allowing information to be shared across all model components.\n\n[^2]: Fast, robust, frequentist left censored multilevel models are not really [possible/available](/posts/left-censored-models-showcase/).\n\nTherefore, a unified model that simultaneously estimates the technical variation and performs the imputation using batch-specific LOD information is better able to obtain unbiased estimates of both the true underlying metabolite levels and the relationships with outcomes or exposures. The `brms` model presented below is a way to implement such a unified model.\n\n# A {brms} model\n\n::: {.cell}\n\n```{.r .cell-code}\nnr_imputations <- 100\nm <- brms::brm(formula = log_y_obs | cens(left_cens) ~ 1 + x + (1|batch_id/plate_nested_coding), \n               data = df |> dplyr::mutate(left_cens = censored * -1), \n               cores = 4, seed = 2025, refresh = 1000,\n               backend = 'cmdstanr')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 60.5 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 61.0 seconds.\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 63.4 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 66.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 62.8 seconds.\nTotal execution time: 66.6 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal_nr_draws <- brms::ndraws(m) \nsamples_to_draw <- sort(sample(total_nr_draws, nr_imputations))\nsigma_draws <- as.data.frame(m, variable = c(\"sigma\"), draw = samples_to_draw) |> dplyr::pull()\n\n# do not include technical variation in draws (= implicitly removing it)\nmu_draws <- brms::posterior_linpred(m, re_formula = NA, draw_ids = samples_to_draw) |> t() \n\nre_draws <- brms::posterior_linpred(m, re_formula = NULL, draw_ids = samples_to_draw) - \n  brms::posterior_linpred(m, re_formula = NA, draw_ids = samples_to_draw) \nre_draws <- t(re_draws)\n\nimputed_sets <- matrix(nrow = length(y), ncol = nr_imputations)\n\nfor(i in 1:nr_imputations) {\n  mu <- mu_draws[, i]\n  sd <- sigma_draws[i]\n  \n  batch_effect <- re_draws[, i]\n  lod_adjusted <- df[['log_lod']] - batch_effect\n  \n  # draw values below lod\n  ## what percentiles belong to start with these params\n  unif_lower <- pnorm(-Inf, mu, sd)\n  unif_upper <- pnorm(lod_adjusted,  mu, sd)\n  ## draw random percentiles between lower and upper\n  rand_perc_interval <- runif(length(mu),  unif_lower, unif_upper) \n  ## get value belonging to percentile\n  imputes <- qnorm(rand_perc_interval, mu, sd) \n  # remove technical variation from detected values\n  y_adjusted <- df[['log_y_obs']] - batch_effect\n  \n  imputed_sets[,i] <- ifelse(df[['censored']], imputes, y_adjusted)\n}\n\nr <- tibble::as_tibble(imputed_sets)\n```\n:::\n\n\nBelow we see that the model has indeed recovered the parameters of the data generating process. The imputations are based on these parameters. \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_y_obs | cens(left_cens) ~ 1 + x + (1 | batch_id/plate_nested_coding) \n   Data: dplyr::mutate(df, left_cens = censored * -1) (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~batch_id (Number of levels: 5) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.98      0.77     0.98     3.96 1.00     1647     1743\n\n~batch_id:plate_nested_coding (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.60      0.13     0.41     0.89 1.00     1333     2191\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    11.49      0.86     9.80    13.26 1.00     1791     2050\nx             0.07      0.02     0.02     0.11 1.00     5030     2630\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.01      0.02     0.97     1.05 1.00     4824     2472\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\nVisually, we can plot one of the imputed sets, and we can see that it fits well with the ideal data distribution that has the technical variation removed and the nondetects imputed. We imputed multiple times to represent and capture the uncertainty around the exact value of the nondetects[^3]. We can also see that some of the observed values are now under the the original limit of detection. This is because -- according to our model -- these observations would have been nondetects in a 'usual' batch. And vice versa, there are some nondetects that would have been observed in a 'usual' batch and are now above the original LOD. \n\n[^3]: In discussions I get the sense that many people think that the goal of imputation in metabolomics is to get as close as possible to the value that we would have observed with unlimited instrument sensitivity. While that's welcome, it's more important for valid downstream inference with censored data to accurately represent the uncertainty associated with those unobserved values. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf_imputes <- dplyr::bind_cols(df, r)\n\np_a <- df_imputes |> \n  ggplot(aes(inj_order, V2)) + \n  scale_color_manual(values = unname(bluebrown_colors_list)) +\n  geom_point(aes(color = batch_id)) + \n  geom_point(data = subset(df_imputes, censored), \n           aes(x = inj_order, y = V2), shape = 5) +\n  labs(x = x_lab, y = y_lab) +\n  batch_lod_hline(df_imputes, nr_batches, samples_p_batch,\n                  linewidth = linewidth, linetype = 'dashed') +\n  ylim(c(min(df$log_y), max(df$log_y))) +\n  theme_minimal() +\n  theme(legend.position = 'none') \n\np_b <- df_imputes |> \n  ggplot(aes(x = V2)) +\n  geom_histogram(bins = nr_bins, fill = 'black', alpha = 0.7, color = 'white') +\n  xlim(c(min(df$log_y), max(df$log_y))) +\n  ylim(c(0, axis_range_bins)) +\n  geom_vline(aes(xintercept = log_lod), linetype = 'dashed', linewidth = linewidth - 0.3) +\n  coord_flip() +\n  theme_void()\n\np_a + p_b + plot_layout(ncol = 2, nrow = 1, widths = c(3, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png)\n:::\n:::\n\nAnd if we plot the values from an imputed set vs the original values we see the increased uncertainty in values below the LOD:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf_imputes |> \n  dplyr::filter(plate_nested_coding == '4_4') |> \n  ggplot(aes(V2, log_y, color = censored)) + \n  geom_point() + \n  geom_hline(aes(yintercept = log_lod), linetype = 'dashed') +\n  see::scale_color_okabeito(order = c(2,1)) +\n  labs(x = 'Original value', y = 'An imputed set value') +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png)\n:::\n:::\n\n\n# Implications\nUnfortunately,  I think this intertwined nature of nondetects and batch effects also means that your technical variation adjustment needs to be bespoke; you cannot do this imputation and 'denoising' once for your dataset and be done with it (if you care to this degree about bias). In theory, you could build the richest possible imputation model (and thus technical variation removal model) that is congenial with the most complex analysis that you would like to do and be done with it. In practice, this would be very hard with the amount of times a dataset is reanalysed by different analysts and the near certainty of missing values for the other variables in a rich imputation model. It will be even harder to create an congenial dataset when a mixture is the topic of interest and the dataset contains many left censored features, because you would need to impute in two variables simultaneously and there's no (easy) way to fit a multivariate left censored model [^4].\n\n[^4]: I think some kind of Gibbs sampler or some chained equation is the best we can do.\n\nOne potential way around this intertwined nature may be using the pooled QC samples for the batch adjustment using some kind of LOESS fit across the injection order. However, (1) labs do not always deliver information on QCs (2) there are less QA samples than study samples and as such there's less data to inform such a model (3) readily available LOESS models do not take left censoring into account. More importantly, it also doesn't easily incorporate other covariates in the adjustment process which risks removing real biological variation and potentially failing to capture that for example cases are more often censored. Such a LOESS approach also still separates batch correction from the imputation of study samples, failing to address this interdependence between batch-specific LODs and the imputation process itself. \n\n# Possible extensions\nSometimes there's also drift within a plate or a batch. One possible way to model this is by adding a random slope for injection order within a batch or plate. The code from above readily supports this, because the code assumes all the random effects are nuisance variation. Although, I think that such more complicated models are much harder to fit so you may want to represent this drift in another way. Maybe using some fixed effects[^5] and then remove this from the imputations and adjustment manually using some `model.matrix()` operation. \n\nOne final point that I want to make on these models, is on the assumption of a multiplicative error term -- here made by the log transformation -- in metabolomic data. It's a useful model in the sense that it permits only positive values and it is in line with our idea of the data generation process. Namely, it is plausible that the true error structure arises from such a multiplicative process, because measured intensity/concentration can be thought of as a function of *duration of exposure* $\\times$ *number of times exposed*. However, in a lognormal distribution the error is also smaller for smaller values. And this is of course opposite of the limit of detection idea. If the error were indeed smaller for smaller values there would be no limit of detection! I hope the multiple versions of the imputed dataset at least overcome this partially. @senn2012ghosts mention that realistic likelihood models could consider a complex mixture of true concentration and error measurement models, or model the mean and variance structures separately. While I always like to think about the ideal model, I think for large scale metabolomics where you often have to fit this model to thousands of features this is not feasible. \n\n<!-- \" the measured concentration can be thought of as a function of *duration of exposure* $\\times$ *number of times exposed*. So differences in measured concentrations of the same compound (i.e. the variation) arise because of differences in one (or both) of the terms of this formula. And as we can see this formula is a *product* of factors, logarithms tend to work better when data arises from such a multiplicative process.\" -->\n\n[^5]: I'm not entirely sure what's better (in this setup): a model with random slopes that possibly doesn't fully converge or some 20 fixed effect interaction terms (with potentially some sparsity inducing horseshoe or LASSO priors). I guess putting (very) strong priors on the random slopes is also a good option.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}